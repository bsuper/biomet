{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CH4 Cross Validation\n",
    "\n",
    "### Train Features\n",
    "1. land surface temp (wp_LST.day)\n",
    "2. sensible heat flux (wp_le)\n",
    "3. latent heat flux (wp_h)\n",
    "4. net radiation (net_rad)\n",
    "5. avg air temp (avg_air_temp)\n",
    "\n",
    "### Performance\n",
    "Compared to regressions on other values, CH4 Methane Regression performs poorly. Looking at the feature correlation plots, we see that there aren't any variables that are strongly correlated with ch4_gf. Thus, this poor performance is not surprising."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "import exp\n",
    "import regression as r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PET</th>\n",
       "      <th>VPD</th>\n",
       "      <th>air_temp</th>\n",
       "      <th>doy</th>\n",
       "      <th>precip</th>\n",
       "      <th>soil_temp</th>\n",
       "      <th>sw_in</th>\n",
       "      <th>wind_speed</th>\n",
       "      <th>year</th>\n",
       "      <th>wp_RNET</th>\n",
       "      <th>...</th>\n",
       "      <th>wp_evi</th>\n",
       "      <th>wp_lswi2</th>\n",
       "      <th>wp_ndvi</th>\n",
       "      <th>mb_evi</th>\n",
       "      <th>mb_lswi2</th>\n",
       "      <th>mb_ndvi</th>\n",
       "      <th>wp_LST.day</th>\n",
       "      <th>wp_LST.night</th>\n",
       "      <th>mb_LST.day</th>\n",
       "      <th>mb_LST.night</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.33</td>\n",
       "      <td>0.808731</td>\n",
       "      <td>19.179167</td>\n",
       "      <td>195</td>\n",
       "      <td>0.0</td>\n",
       "      <td>22.320833</td>\n",
       "      <td>30.3156</td>\n",
       "      <td>4.958333</td>\n",
       "      <td>2012</td>\n",
       "      <td>20.798342</td>\n",
       "      <td>...</td>\n",
       "      <td>0.355407</td>\n",
       "      <td>0.286584</td>\n",
       "      <td>0.611743</td>\n",
       "      <td>0.278104</td>\n",
       "      <td>0.523764</td>\n",
       "      <td>0.652612</td>\n",
       "      <td>31.567899</td>\n",
       "      <td>17.204530</td>\n",
       "      <td>26.696193</td>\n",
       "      <td>18.481563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6.52</td>\n",
       "      <td>0.755945</td>\n",
       "      <td>19.325000</td>\n",
       "      <td>196</td>\n",
       "      <td>0.0</td>\n",
       "      <td>21.770833</td>\n",
       "      <td>29.6316</td>\n",
       "      <td>3.791667</td>\n",
       "      <td>2012</td>\n",
       "      <td>20.573593</td>\n",
       "      <td>...</td>\n",
       "      <td>0.362843</td>\n",
       "      <td>0.317110</td>\n",
       "      <td>0.624457</td>\n",
       "      <td>0.281016</td>\n",
       "      <td>0.525663</td>\n",
       "      <td>0.651848</td>\n",
       "      <td>29.570000</td>\n",
       "      <td>17.390000</td>\n",
       "      <td>26.190000</td>\n",
       "      <td>18.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6.92</td>\n",
       "      <td>0.858993</td>\n",
       "      <td>20.262500</td>\n",
       "      <td>197</td>\n",
       "      <td>0.0</td>\n",
       "      <td>21.908333</td>\n",
       "      <td>29.3472</td>\n",
       "      <td>4.137500</td>\n",
       "      <td>2012</td>\n",
       "      <td>20.475931</td>\n",
       "      <td>...</td>\n",
       "      <td>0.370279</td>\n",
       "      <td>0.347637</td>\n",
       "      <td>0.637171</td>\n",
       "      <td>0.283928</td>\n",
       "      <td>0.527563</td>\n",
       "      <td>0.651084</td>\n",
       "      <td>31.097908</td>\n",
       "      <td>17.235624</td>\n",
       "      <td>26.745817</td>\n",
       "      <td>18.494425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6.35</td>\n",
       "      <td>0.477617</td>\n",
       "      <td>16.791667</td>\n",
       "      <td>198</td>\n",
       "      <td>0.0</td>\n",
       "      <td>22.420833</td>\n",
       "      <td>28.8180</td>\n",
       "      <td>6.033333</td>\n",
       "      <td>2012</td>\n",
       "      <td>20.571045</td>\n",
       "      <td>...</td>\n",
       "      <td>0.377714</td>\n",
       "      <td>0.378163</td>\n",
       "      <td>0.649886</td>\n",
       "      <td>0.286840</td>\n",
       "      <td>0.529463</td>\n",
       "      <td>0.650320</td>\n",
       "      <td>30.868718</td>\n",
       "      <td>17.248525</td>\n",
       "      <td>26.769170</td>\n",
       "      <td>18.499213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.13</td>\n",
       "      <td>0.556820</td>\n",
       "      <td>17.016667</td>\n",
       "      <td>199</td>\n",
       "      <td>0.0</td>\n",
       "      <td>21.529167</td>\n",
       "      <td>23.1732</td>\n",
       "      <td>4.350000</td>\n",
       "      <td>2012</td>\n",
       "      <td>16.757401</td>\n",
       "      <td>...</td>\n",
       "      <td>0.385150</td>\n",
       "      <td>0.408689</td>\n",
       "      <td>0.662600</td>\n",
       "      <td>0.289752</td>\n",
       "      <td>0.531362</td>\n",
       "      <td>0.649556</td>\n",
       "      <td>30.657792</td>\n",
       "      <td>17.259663</td>\n",
       "      <td>26.791436</td>\n",
       "      <td>18.502905</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 33 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    PET       VPD   air_temp  doy  precip  soil_temp    sw_in  wind_speed  \\\n",
       "0  7.33  0.808731  19.179167  195     0.0  22.320833  30.3156    4.958333   \n",
       "1  6.52  0.755945  19.325000  196     0.0  21.770833  29.6316    3.791667   \n",
       "2  6.92  0.858993  20.262500  197     0.0  21.908333  29.3472    4.137500   \n",
       "3  6.35  0.477617  16.791667  198     0.0  22.420833  28.8180    6.033333   \n",
       "4  5.13  0.556820  17.016667  199     0.0  21.529167  23.1732    4.350000   \n",
       "\n",
       "   year    wp_RNET      ...         wp_evi  wp_lswi2   wp_ndvi    mb_evi  \\\n",
       "0  2012  20.798342      ...       0.355407  0.286584  0.611743  0.278104   \n",
       "1  2012  20.573593      ...       0.362843  0.317110  0.624457  0.281016   \n",
       "2  2012  20.475931      ...       0.370279  0.347637  0.637171  0.283928   \n",
       "3  2012  20.571045      ...       0.377714  0.378163  0.649886  0.286840   \n",
       "4  2012  16.757401      ...       0.385150  0.408689  0.662600  0.289752   \n",
       "\n",
       "   mb_lswi2   mb_ndvi  wp_LST.day  wp_LST.night  mb_LST.day  mb_LST.night  \n",
       "0  0.523764  0.652612   31.567899     17.204530   26.696193     18.481563  \n",
       "1  0.525663  0.651848   29.570000     17.390000   26.190000     18.750000  \n",
       "2  0.527563  0.651084   31.097908     17.235624   26.745817     18.494425  \n",
       "3  0.529463  0.650320   30.868718     17.248525   26.769170     18.499213  \n",
       "4  0.531362  0.649556   30.657792     17.259663   26.791436     18.502905  \n",
       "\n",
       "[5 rows x 33 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = exp.get_exp1_data()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([u'PET', u'VPD', u'air_temp', u'doy', u'precip', u'soil_temp', u'sw_in',\n",
       "       u'wind_speed', u'year', u'wp_RNET', u'wp_ch4_gf', u'wp_co2_gf',\n",
       "       u'wp_er', u'wp_gpp', u'wp_h', u'wp_le', u'mb_RNET', u'mb_ch4_gf',\n",
       "       u'mb_co2_gf', u'mb_er', u'mb_gpp', u'mb_h', u'mb_le', u'wp_evi',\n",
       "       u'wp_lswi2', u'wp_ndvi', u'mb_evi', u'mb_lswi2', u'mb_ndvi',\n",
       "       u'wp_LST.day', u'wp_LST.night', u'mb_LST.day', u'mb_LST.night'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1028, 5)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_cols = [\"wp_LST.day\", \"wp_h\", \"wp_le\", \"wp_RNET\", \"air_temp\"]\n",
    "X, Y = exp.featurize(df, train_cols, [\"wp_ch4_gf\"])\n",
    "X, Y, scaler = r.preprocess(X, Y)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Random Forests Cross Validation...\n",
      "10-fold CV Acc Mean:  0.691183840697\n",
      "CV Scores:  0.655134405198, 0.709847839905, 0.720787410836, 0.818368552438, 0.689200411957, 0.70668138638, 0.66424639531, 0.626592679601, 0.542842188584, 0.778137136764\n",
      "OOB score: 0.699727734529\n",
      "Feature Importances:\n",
      "('wp_LST.day', 0.25284529815795304)\n",
      "('air_temp', 0.24555207801640619)\n",
      "('wp_le', 0.21920507114116972)\n",
      "('wp_RNET', 0.17084502200593576)\n",
      "('wp_h', 0.11155253067853538)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
       "           max_features='sqrt', max_leaf_nodes=None, min_samples_leaf=1,\n",
       "           min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "           n_estimators=200, n_jobs=1, oob_score=True, random_state=None,\n",
       "           verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.random_forests_cross_val(X, Y, feature_names=train_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Gradient Boosted Trees Cross Validation...\n",
      "10-fold CV Acc Mean:  0.666788194531\n",
      "CV Scores:  0.670432584266, 0.693631169775, 0.664412257737, 0.770229921657, 0.651586839893, 0.701751195374, 0.621980191269, 0.630878579378, 0.501944322098, 0.761034883866\n",
      "Feature Importances:\n",
      "('wp_le', 0.23435178134481738)\n",
      "('wp_LST.day', 0.21417358330992708)\n",
      "('wp_RNET', 0.19292231108792815)\n",
      "('air_temp', 0.18776145568736546)\n",
      "('wp_h', 0.17079086856996187)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GradientBoostingRegressor(alpha=0.9, init=None, learning_rate=0.1, loss='ls',\n",
       "             max_depth=3, max_features='sqrt', max_leaf_nodes=None,\n",
       "             min_samples_leaf=1, min_samples_split=2,\n",
       "             min_weight_fraction_leaf=0.0, n_estimators=200,\n",
       "             presort='auto', random_state=None, subsample=1.0, verbose=0,\n",
       "             warm_start=False)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.xgb_trees_cross_val(X, Y, feature_names=train_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running SVC Cross Validation...\n",
      "10-fold CV Acc Mean:  0.079506659563\n",
      "CV Scores:  0.100564145999, -0.0518856226057, 0.126556489416, 0.131980847778, 0.012326960445, 0.144847570161, 0.0777971709179, 0.072742383218, 0.106896975573, 0.0732396747264\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SVR(C=1.0, cache_size=200, coef0=0.0, degree=3, epsilon=0.1, gamma='auto',\n",
       "  kernel='rbf', max_iter=-1, shrinking=True, tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.svc_cross_val(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Neural Network Cross Validation...\n",
      "Step #100, epoch #10, avg. train loss: 681970.81250\n",
      "Step #200, epoch #20, avg. train loss: 655331.37500\n",
      "Step #300, epoch #30, avg. train loss: 633762.06250\n",
      "Step #400, epoch #40, avg. train loss: 606714.18750\n",
      "Step #500, epoch #50, avg. train loss: 605515.37500\n",
      "Step #600, epoch #60, avg. train loss: 595052.50000\n",
      "Step #700, epoch #70, avg. train loss: 567497.56250\n",
      "Step #800, epoch #80, avg. train loss: 576539.18750\n",
      "Step #900, epoch #90, avg. train loss: 570689.50000\n",
      "Step #1000, epoch #100, avg. train loss: 547691.75000\n",
      "Step #1100, epoch #110, avg. train loss: 546829.62500\n",
      "Step #1200, epoch #120, avg. train loss: 537669.62500\n",
      "Step #1300, epoch #130, avg. train loss: 541272.25000\n",
      "Step #1400, epoch #140, avg. train loss: 531041.81250\n",
      "Step #1500, epoch #150, avg. train loss: 525890.00000\n",
      "Step #1600, epoch #160, avg. train loss: 515687.84375\n",
      "Step #1700, epoch #170, avg. train loss: 504061.62500\n",
      "Step #1800, epoch #180, avg. train loss: 507499.12500\n",
      "Step #1900, epoch #190, avg. train loss: 498328.87500\n",
      "Step #2000, epoch #200, avg. train loss: 487658.09375\n",
      "Step #2100, epoch #210, avg. train loss: 487805.37500\n",
      "Step #2200, epoch #220, avg. train loss: 485823.46875\n",
      "Step #2300, epoch #230, avg. train loss: 480632.31250\n",
      "Step #2400, epoch #240, avg. train loss: 474603.15625\n",
      "Step #2500, epoch #250, avg. train loss: 476244.71875\n",
      "Step #2600, epoch #260, avg. train loss: 467934.25000\n",
      "Step #2700, epoch #270, avg. train loss: 462030.25000\n",
      "Step #2800, epoch #280, avg. train loss: 468145.62500\n",
      "Step #2900, epoch #290, avg. train loss: 453754.28125\n",
      "Step #3000, epoch #300, avg. train loss: 454200.37500\n",
      "Step #3100, epoch #310, avg. train loss: 442657.59375\n",
      "Step #3200, epoch #320, avg. train loss: 447905.56250\n",
      "Step #3300, epoch #330, avg. train loss: 439384.00000\n",
      "Step #3400, epoch #340, avg. train loss: 440608.68750\n",
      "Step #3500, epoch #350, avg. train loss: 431365.84375\n",
      "Step #3600, epoch #360, avg. train loss: 434855.56250\n",
      "Step #3700, epoch #370, avg. train loss: 424880.53125\n",
      "Step #3800, epoch #380, avg. train loss: 432940.87500\n",
      "Step #3900, epoch #390, avg. train loss: 429169.56250\n",
      "Step #4000, epoch #400, avg. train loss: 421892.71875\n",
      "Step #4100, epoch #410, avg. train loss: 423668.25000\n",
      "Step #4200, epoch #420, avg. train loss: 418149.75000\n",
      "Step #4300, epoch #430, avg. train loss: 418115.75000\n",
      "Step #4400, epoch #440, avg. train loss: 408931.03125\n",
      "Step #4500, epoch #450, avg. train loss: 409386.53125\n",
      "Step #4600, epoch #460, avg. train loss: 407439.12500\n",
      "Step #4700, epoch #470, avg. train loss: 407906.90625\n",
      "Step #4800, epoch #480, avg. train loss: 398898.87500\n",
      "Step #4900, epoch #490, avg. train loss: 395726.00000\n",
      "Step #5000, epoch #500, avg. train loss: 390759.87500\n",
      "Step #100, epoch #10, avg. train loss: 705952.37500\n",
      "Step #200, epoch #20, avg. train loss: 667078.81250\n",
      "Step #300, epoch #30, avg. train loss: 654603.87500\n",
      "Step #400, epoch #40, avg. train loss: 633941.25000\n",
      "Step #500, epoch #50, avg. train loss: 617048.68750\n",
      "Step #600, epoch #60, avg. train loss: 608034.25000\n",
      "Step #700, epoch #70, avg. train loss: 589316.62500\n",
      "Step #800, epoch #80, avg. train loss: 593000.06250\n",
      "Step #900, epoch #90, avg. train loss: 578521.81250\n",
      "Step #1000, epoch #100, avg. train loss: 573769.68750\n",
      "Step #1100, epoch #110, avg. train loss: 562537.81250\n",
      "Step #1200, epoch #120, avg. train loss: 554114.18750\n",
      "Step #1300, epoch #130, avg. train loss: 549240.31250\n",
      "Step #1400, epoch #140, avg. train loss: 542949.56250\n",
      "Step #1500, epoch #150, avg. train loss: 532104.06250\n",
      "Step #1600, epoch #160, avg. train loss: 534135.50000\n",
      "Step #1700, epoch #170, avg. train loss: 526114.56250\n",
      "Step #1800, epoch #180, avg. train loss: 515471.56250\n",
      "Step #1900, epoch #190, avg. train loss: 512430.18750\n",
      "Step #2000, epoch #200, avg. train loss: 506327.46875\n",
      "Step #2100, epoch #210, avg. train loss: 499267.75000\n",
      "Step #2200, epoch #220, avg. train loss: 507693.96875\n",
      "Step #2300, epoch #230, avg. train loss: 494367.03125\n",
      "Step #2400, epoch #240, avg. train loss: 494138.96875\n",
      "Step #2500, epoch #250, avg. train loss: 487127.68750\n",
      "Step #2600, epoch #260, avg. train loss: 487330.62500\n",
      "Step #2700, epoch #270, avg. train loss: 476739.00000\n",
      "Step #2800, epoch #280, avg. train loss: 470736.68750\n",
      "Step #2900, epoch #290, avg. train loss: 469150.31250\n",
      "Step #3000, epoch #300, avg. train loss: 462406.71875\n",
      "Step #3100, epoch #310, avg. train loss: 465930.37500\n",
      "Step #3200, epoch #320, avg. train loss: 456671.84375\n",
      "Step #3300, epoch #330, avg. train loss: 462610.28125\n",
      "Step #3400, epoch #340, avg. train loss: 452034.15625\n",
      "Step #3500, epoch #350, avg. train loss: 452468.96875\n",
      "Step #3600, epoch #360, avg. train loss: 452892.71875\n",
      "Step #3700, epoch #370, avg. train loss: 449989.40625\n",
      "Step #3800, epoch #380, avg. train loss: 437403.40625\n",
      "Step #3900, epoch #390, avg. train loss: 436036.46875\n",
      "Step #4000, epoch #400, avg. train loss: 436625.28125\n",
      "Step #4100, epoch #410, avg. train loss: 431806.00000\n",
      "Step #4200, epoch #420, avg. train loss: 430531.96875\n",
      "Step #4300, epoch #430, avg. train loss: 424952.37500\n",
      "Step #4400, epoch #440, avg. train loss: 431225.96875\n",
      "Step #4500, epoch #450, avg. train loss: 420336.40625\n",
      "Step #4600, epoch #460, avg. train loss: 423472.43750\n",
      "Step #4700, epoch #470, avg. train loss: 418306.59375\n",
      "Step #4800, epoch #480, avg. train loss: 417152.15625\n",
      "Step #4900, epoch #490, avg. train loss: 416125.81250\n",
      "Step #5000, epoch #500, avg. train loss: 411177.00000\n",
      "Step #100, epoch #10, avg. train loss: 696877.12500\n",
      "Step #200, epoch #20, avg. train loss: 660497.37500\n",
      "Step #300, epoch #30, avg. train loss: 651033.75000\n",
      "Step #400, epoch #40, avg. train loss: 621727.75000\n",
      "Step #500, epoch #50, avg. train loss: 605069.75000\n",
      "Step #600, epoch #60, avg. train loss: 599981.25000\n",
      "Step #700, epoch #70, avg. train loss: 579776.75000\n",
      "Step #800, epoch #80, avg. train loss: 585824.37500\n",
      "Step #900, epoch #90, avg. train loss: 572939.37500\n",
      "Step #1000, epoch #100, avg. train loss: 564773.93750\n",
      "Step #1100, epoch #110, avg. train loss: 559180.18750\n",
      "Step #1200, epoch #120, avg. train loss: 551270.43750\n",
      "Step #1300, epoch #130, avg. train loss: 545848.93750\n",
      "Step #1400, epoch #140, avg. train loss: 534249.56250\n",
      "Step #1500, epoch #150, avg. train loss: 527701.81250\n",
      "Step #1600, epoch #160, avg. train loss: 524337.37500\n",
      "Step #1700, epoch #170, avg. train loss: 530132.62500\n",
      "Step #1800, epoch #180, avg. train loss: 509429.37500\n",
      "Step #1900, epoch #190, avg. train loss: 510528.12500\n",
      "Step #2000, epoch #200, avg. train loss: 503594.28125\n",
      "Step #2100, epoch #210, avg. train loss: 497345.56250\n",
      "Step #2200, epoch #220, avg. train loss: 498665.18750\n",
      "Step #2300, epoch #230, avg. train loss: 493463.03125\n",
      "Step #2400, epoch #240, avg. train loss: 488220.37500\n",
      "Step #2500, epoch #250, avg. train loss: 487282.56250\n",
      "Step #2600, epoch #260, avg. train loss: 484719.84375\n",
      "Step #2700, epoch #270, avg. train loss: 479842.81250\n",
      "Step #2800, epoch #280, avg. train loss: 470779.28125\n",
      "Step #2900, epoch #290, avg. train loss: 460989.37500\n",
      "Step #3000, epoch #300, avg. train loss: 465504.43750\n",
      "Step #3100, epoch #310, avg. train loss: 463783.40625\n",
      "Step #3200, epoch #320, avg. train loss: 463794.37500\n",
      "Step #3300, epoch #330, avg. train loss: 453692.75000\n",
      "Step #3400, epoch #340, avg. train loss: 446107.40625\n",
      "Step #3500, epoch #350, avg. train loss: 444339.56250\n",
      "Step #3600, epoch #360, avg. train loss: 448728.81250\n",
      "Step #3700, epoch #370, avg. train loss: 441530.56250\n",
      "Step #3800, epoch #380, avg. train loss: 430611.84375\n",
      "Step #3900, epoch #390, avg. train loss: 434440.28125\n",
      "Step #4000, epoch #400, avg. train loss: 423670.96875\n",
      "Step #4100, epoch #410, avg. train loss: 435472.59375\n",
      "Step #4200, epoch #420, avg. train loss: 425559.37500\n",
      "Step #4300, epoch #430, avg. train loss: 418288.00000\n",
      "Step #4400, epoch #440, avg. train loss: 423419.12500\n",
      "Step #4500, epoch #450, avg. train loss: 413444.71875\n",
      "Step #4600, epoch #460, avg. train loss: 418045.59375\n",
      "Step #4700, epoch #470, avg. train loss: 408020.25000\n",
      "Step #4800, epoch #480, avg. train loss: 406469.15625\n",
      "Step #4900, epoch #490, avg. train loss: 404290.75000\n",
      "Step #5000, epoch #500, avg. train loss: 403096.03125\n",
      "Step #100, epoch #10, avg. train loss: 687068.81250\n",
      "Step #200, epoch #20, avg. train loss: 664572.93750\n",
      "Step #300, epoch #30, avg. train loss: 650367.18750\n",
      "Step #400, epoch #40, avg. train loss: 621560.50000\n",
      "Step #500, epoch #50, avg. train loss: 609152.62500\n",
      "Step #600, epoch #60, avg. train loss: 600147.93750\n",
      "Step #700, epoch #70, avg. train loss: 584998.06250\n",
      "Step #800, epoch #80, avg. train loss: 586026.93750\n",
      "Step #900, epoch #90, avg. train loss: 568034.56250\n",
      "Step #1000, epoch #100, avg. train loss: 559755.50000\n",
      "Step #1100, epoch #110, avg. train loss: 557864.00000\n",
      "Step #1200, epoch #120, avg. train loss: 547898.31250\n",
      "Step #1300, epoch #130, avg. train loss: 536193.81250\n",
      "Step #1400, epoch #140, avg. train loss: 542744.81250\n",
      "Step #1500, epoch #150, avg. train loss: 528640.31250\n",
      "Step #1600, epoch #160, avg. train loss: 522635.71875\n",
      "Step #1700, epoch #170, avg. train loss: 520248.28125\n",
      "Step #1800, epoch #180, avg. train loss: 510560.62500\n",
      "Step #1900, epoch #190, avg. train loss: 508331.15625\n",
      "Step #2000, epoch #200, avg. train loss: 502149.68750\n",
      "Step #2100, epoch #210, avg. train loss: 495040.37500\n",
      "Step #2200, epoch #220, avg. train loss: 491461.40625\n",
      "Step #2300, epoch #230, avg. train loss: 491590.09375\n",
      "Step #2400, epoch #240, avg. train loss: 480469.62500\n",
      "Step #2500, epoch #250, avg. train loss: 482648.28125\n",
      "Step #2600, epoch #260, avg. train loss: 475560.87500\n",
      "Step #2700, epoch #270, avg. train loss: 465441.81250\n",
      "Step #2800, epoch #280, avg. train loss: 471573.37500\n",
      "Step #2900, epoch #290, avg. train loss: 467448.03125\n",
      "Step #3000, epoch #300, avg. train loss: 453835.81250\n",
      "Step #3100, epoch #310, avg. train loss: 451740.03125\n",
      "Step #3200, epoch #320, avg. train loss: 451346.81250\n",
      "Step #3300, epoch #330, avg. train loss: 448833.18750\n",
      "Step #3400, epoch #340, avg. train loss: 444532.84375\n",
      "Step #3500, epoch #350, avg. train loss: 440553.03125\n",
      "Step #3600, epoch #360, avg. train loss: 443055.31250\n",
      "Step #3700, epoch #370, avg. train loss: 439524.12500\n",
      "Step #3800, epoch #380, avg. train loss: 432724.46875\n",
      "Step #3900, epoch #390, avg. train loss: 432573.43750\n",
      "Step #4000, epoch #400, avg. train loss: 423769.84375\n",
      "Step #4100, epoch #410, avg. train loss: 424163.43750\n",
      "Step #4200, epoch #420, avg. train loss: 428338.68750\n",
      "Step #4300, epoch #430, avg. train loss: 423027.84375\n",
      "Step #4400, epoch #440, avg. train loss: 420435.84375\n",
      "Step #4500, epoch #450, avg. train loss: 411987.68750\n",
      "Step #4600, epoch #460, avg. train loss: 416424.43750\n",
      "Step #4700, epoch #470, avg. train loss: 415951.18750\n",
      "Step #4800, epoch #480, avg. train loss: 404088.56250\n",
      "Step #4900, epoch #490, avg. train loss: 400702.81250\n",
      "Step #5000, epoch #500, avg. train loss: 399384.59375\n",
      "Step #100, epoch #10, avg. train loss: 679839.75000\n",
      "Step #200, epoch #20, avg. train loss: 662132.43750\n",
      "Step #300, epoch #30, avg. train loss: 628130.93750\n",
      "Step #400, epoch #40, avg. train loss: 618097.93750\n",
      "Step #500, epoch #50, avg. train loss: 594078.81250\n",
      "Step #600, epoch #60, avg. train loss: 597040.43750\n",
      "Step #700, epoch #70, avg. train loss: 585575.56250\n",
      "Step #800, epoch #80, avg. train loss: 571103.62500\n",
      "Step #900, epoch #90, avg. train loss: 560317.50000\n",
      "Step #1000, epoch #100, avg. train loss: 556886.93750\n",
      "Step #1100, epoch #110, avg. train loss: 556124.81250\n",
      "Step #1200, epoch #120, avg. train loss: 543409.93750\n",
      "Step #1300, epoch #130, avg. train loss: 537038.06250\n",
      "Step #1400, epoch #140, avg. train loss: 521728.68750\n",
      "Step #1500, epoch #150, avg. train loss: 513756.56250\n",
      "Step #1600, epoch #160, avg. train loss: 520050.59375\n",
      "Step #1700, epoch #170, avg. train loss: 502882.75000\n",
      "Step #1800, epoch #180, avg. train loss: 500893.87500\n",
      "Step #1900, epoch #190, avg. train loss: 494128.15625\n",
      "Step #2000, epoch #200, avg. train loss: 496490.71875\n",
      "Step #2100, epoch #210, avg. train loss: 490718.96875\n",
      "Step #2200, epoch #220, avg. train loss: 488900.81250\n",
      "Step #2300, epoch #230, avg. train loss: 475430.46875\n",
      "Step #2400, epoch #240, avg. train loss: 476629.09375\n",
      "Step #2500, epoch #250, avg. train loss: 480445.75000\n",
      "Step #2600, epoch #260, avg. train loss: 466341.03125\n",
      "Step #2700, epoch #270, avg. train loss: 462555.56250\n",
      "Step #2800, epoch #280, avg. train loss: 464002.53125\n",
      "Step #2900, epoch #290, avg. train loss: 457164.31250\n",
      "Step #3000, epoch #300, avg. train loss: 454385.28125\n",
      "Step #3100, epoch #310, avg. train loss: 447530.53125\n",
      "Step #3200, epoch #320, avg. train loss: 456732.25000\n",
      "Step #3300, epoch #330, avg. train loss: 449563.90625\n",
      "Step #3400, epoch #340, avg. train loss: 444482.18750\n",
      "Step #3500, epoch #350, avg. train loss: 436275.62500\n",
      "Step #3600, epoch #360, avg. train loss: 433170.62500\n",
      "Step #3700, epoch #370, avg. train loss: 440861.31250\n",
      "Step #3800, epoch #380, avg. train loss: 428720.75000\n",
      "Step #3900, epoch #390, avg. train loss: 422041.25000\n",
      "Step #4000, epoch #400, avg. train loss: 431200.18750\n",
      "Step #4100, epoch #410, avg. train loss: 429310.75000\n",
      "Step #4200, epoch #420, avg. train loss: 416699.28125\n",
      "Step #4300, epoch #430, avg. train loss: 416011.71875\n",
      "Step #4400, epoch #440, avg. train loss: 417034.15625\n",
      "Step #4500, epoch #450, avg. train loss: 414703.59375\n",
      "Step #4600, epoch #460, avg. train loss: 407012.53125\n",
      "Step #4700, epoch #470, avg. train loss: 402330.03125\n",
      "Step #4800, epoch #480, avg. train loss: 398469.62500\n",
      "Step #4900, epoch #490, avg. train loss: 398851.81250\n",
      "Step #5000, epoch #500, avg. train loss: 396466.12500\n",
      "Step #100, epoch #10, avg. train loss: 668253.06250\n",
      "Step #200, epoch #20, avg. train loss: 638398.25000\n",
      "Step #300, epoch #30, avg. train loss: 618425.87500\n",
      "Step #400, epoch #40, avg. train loss: 598485.25000\n",
      "Step #500, epoch #50, avg. train loss: 590730.75000\n",
      "Step #600, epoch #60, avg. train loss: 578996.18750\n",
      "Step #700, epoch #70, avg. train loss: 567507.68750\n",
      "Step #800, epoch #80, avg. train loss: 557495.18750\n",
      "Step #900, epoch #90, avg. train loss: 545922.75000\n",
      "Step #1000, epoch #100, avg. train loss: 539296.50000\n",
      "Step #1100, epoch #110, avg. train loss: 538825.50000\n",
      "Step #1200, epoch #120, avg. train loss: 526601.31250\n",
      "Step #1300, epoch #130, avg. train loss: 523675.56250\n",
      "Step #1400, epoch #140, avg. train loss: 511835.75000\n",
      "Step #1500, epoch #150, avg. train loss: 509646.53125\n",
      "Step #1600, epoch #160, avg. train loss: 507740.71875\n",
      "Step #1700, epoch #170, avg. train loss: 497708.68750\n",
      "Step #1800, epoch #180, avg. train loss: 489645.68750\n",
      "Step #1900, epoch #190, avg. train loss: 489382.03125\n",
      "Step #2000, epoch #200, avg. train loss: 479615.03125\n",
      "Step #2100, epoch #210, avg. train loss: 478851.43750\n",
      "Step #2200, epoch #220, avg. train loss: 471378.46875\n",
      "Step #2300, epoch #230, avg. train loss: 468543.37500\n",
      "Step #2400, epoch #240, avg. train loss: 458679.15625\n",
      "Step #2500, epoch #250, avg. train loss: 464128.90625\n",
      "Step #2600, epoch #260, avg. train loss: 454738.62500\n",
      "Step #2700, epoch #270, avg. train loss: 452624.09375\n",
      "Step #2800, epoch #280, avg. train loss: 446782.40625\n",
      "Step #2900, epoch #290, avg. train loss: 449172.37500\n",
      "Step #3000, epoch #300, avg. train loss: 436854.71875\n",
      "Step #3100, epoch #310, avg. train loss: 436014.71875\n",
      "Step #3200, epoch #320, avg. train loss: 442366.46875\n",
      "Step #3300, epoch #330, avg. train loss: 440216.46875\n",
      "Step #3400, epoch #340, avg. train loss: 420949.31250\n",
      "Step #3500, epoch #350, avg. train loss: 431095.37500\n",
      "Step #3600, epoch #360, avg. train loss: 419335.40625\n",
      "Step #3700, epoch #370, avg. train loss: 424879.81250\n",
      "Step #3800, epoch #380, avg. train loss: 415925.12500\n",
      "Step #3900, epoch #390, avg. train loss: 408462.53125\n",
      "Step #4000, epoch #400, avg. train loss: 412372.03125\n",
      "Step #4100, epoch #410, avg. train loss: 408374.12500\n",
      "Step #4200, epoch #420, avg. train loss: 405060.03125\n",
      "Step #4300, epoch #430, avg. train loss: 402305.46875\n",
      "Step #4400, epoch #440, avg. train loss: 401128.68750\n",
      "Step #4500, epoch #450, avg. train loss: 399776.56250\n",
      "Step #4600, epoch #460, avg. train loss: 389282.96875\n",
      "Step #4700, epoch #470, avg. train loss: 394187.59375\n",
      "Step #4800, epoch #480, avg. train loss: 388152.81250\n",
      "Step #4900, epoch #490, avg. train loss: 393108.25000\n",
      "Step #5000, epoch #500, avg. train loss: 386488.15625\n",
      "Step #100, epoch #10, avg. train loss: 690407.50000\n",
      "Step #200, epoch #20, avg. train loss: 661732.62500\n",
      "Step #300, epoch #30, avg. train loss: 643179.93750\n",
      "Step #400, epoch #40, avg. train loss: 626224.06250\n",
      "Step #500, epoch #50, avg. train loss: 611572.25000\n",
      "Step #600, epoch #60, avg. train loss: 599542.25000\n",
      "Step #700, epoch #70, avg. train loss: 586470.12500\n",
      "Step #800, epoch #80, avg. train loss: 578883.25000\n",
      "Step #900, epoch #90, avg. train loss: 576446.75000\n",
      "Step #1000, epoch #100, avg. train loss: 567691.62500\n",
      "Step #1100, epoch #110, avg. train loss: 550611.50000\n",
      "Step #1200, epoch #120, avg. train loss: 551693.25000\n",
      "Step #1300, epoch #130, avg. train loss: 534916.81250\n",
      "Step #1400, epoch #140, avg. train loss: 540723.56250\n",
      "Step #1500, epoch #150, avg. train loss: 528974.18750\n",
      "Step #1600, epoch #160, avg. train loss: 527059.06250\n",
      "Step #1700, epoch #170, avg. train loss: 526147.75000\n",
      "Step #1800, epoch #180, avg. train loss: 516003.84375\n",
      "Step #1900, epoch #190, avg. train loss: 501471.12500\n",
      "Step #2000, epoch #200, avg. train loss: 501890.84375\n",
      "Step #2100, epoch #210, avg. train loss: 499284.53125\n",
      "Step #2200, epoch #220, avg. train loss: 500144.25000\n",
      "Step #2300, epoch #230, avg. train loss: 492773.90625\n",
      "Step #2400, epoch #240, avg. train loss: 475235.53125\n",
      "Step #2500, epoch #250, avg. train loss: 478296.87500\n",
      "Step #2600, epoch #260, avg. train loss: 478792.53125\n",
      "Step #2700, epoch #270, avg. train loss: 476846.03125\n",
      "Step #2800, epoch #280, avg. train loss: 474879.90625\n",
      "Step #2900, epoch #290, avg. train loss: 462118.09375\n",
      "Step #3000, epoch #300, avg. train loss: 462247.56250\n",
      "Step #3100, epoch #310, avg. train loss: 454525.71875\n",
      "Step #3200, epoch #320, avg. train loss: 465129.81250\n",
      "Step #3300, epoch #330, avg. train loss: 451093.71875\n",
      "Step #3400, epoch #340, avg. train loss: 447065.96875\n",
      "Step #3500, epoch #350, avg. train loss: 442718.87500\n",
      "Step #3600, epoch #360, avg. train loss: 441828.12500\n",
      "Step #3700, epoch #370, avg. train loss: 441108.18750\n",
      "Step #3800, epoch #380, avg. train loss: 436430.68750\n",
      "Step #3900, epoch #390, avg. train loss: 435425.00000\n",
      "Step #4000, epoch #400, avg. train loss: 423409.18750\n",
      "Step #4100, epoch #410, avg. train loss: 428687.53125\n",
      "Step #4200, epoch #420, avg. train loss: 427369.81250\n",
      "Step #4300, epoch #430, avg. train loss: 426007.59375\n",
      "Step #4400, epoch #440, avg. train loss: 421552.46875\n",
      "Step #4500, epoch #450, avg. train loss: 413939.40625\n",
      "Step #4600, epoch #460, avg. train loss: 409668.00000\n",
      "Step #4700, epoch #470, avg. train loss: 409165.28125\n",
      "Step #4800, epoch #480, avg. train loss: 407404.09375\n",
      "Step #4900, epoch #490, avg. train loss: 404049.15625\n",
      "Step #5000, epoch #500, avg. train loss: 401453.28125\n",
      "Step #100, epoch #10, avg. train loss: 706650.37500\n",
      "Step #200, epoch #20, avg. train loss: 678754.06250\n",
      "Step #300, epoch #30, avg. train loss: 654902.50000\n",
      "Step #400, epoch #40, avg. train loss: 631312.25000\n",
      "Step #500, epoch #50, avg. train loss: 624284.50000\n",
      "Step #600, epoch #60, avg. train loss: 612902.75000\n",
      "Step #700, epoch #70, avg. train loss: 602807.68750\n",
      "Step #800, epoch #80, avg. train loss: 595204.50000\n",
      "Step #900, epoch #90, avg. train loss: 577161.06250\n",
      "Step #1000, epoch #100, avg. train loss: 582326.06250\n",
      "Step #1100, epoch #110, avg. train loss: 565413.25000\n",
      "Step #1200, epoch #120, avg. train loss: 557440.37500\n",
      "Step #1300, epoch #130, avg. train loss: 556461.06250\n",
      "Step #1400, epoch #140, avg. train loss: 552620.75000\n",
      "Step #1500, epoch #150, avg. train loss: 542626.50000\n",
      "Step #1600, epoch #160, avg. train loss: 531647.18750\n",
      "Step #1700, epoch #170, avg. train loss: 528904.62500\n",
      "Step #1800, epoch #180, avg. train loss: 536450.06250\n",
      "Step #1900, epoch #190, avg. train loss: 522857.96875\n",
      "Step #2000, epoch #200, avg. train loss: 522751.25000\n",
      "Step #2100, epoch #210, avg. train loss: 510892.68750\n",
      "Step #2200, epoch #220, avg. train loss: 498610.15625\n",
      "Step #2300, epoch #230, avg. train loss: 496023.56250\n",
      "Step #2400, epoch #240, avg. train loss: 492429.87500\n",
      "Step #2500, epoch #250, avg. train loss: 498307.90625\n",
      "Step #2600, epoch #260, avg. train loss: 494645.43750\n",
      "Step #2700, epoch #270, avg. train loss: 484497.03125\n",
      "Step #2800, epoch #280, avg. train loss: 478951.71875\n",
      "Step #2900, epoch #290, avg. train loss: 473457.96875\n",
      "Step #3000, epoch #300, avg. train loss: 467786.09375\n",
      "Step #3100, epoch #310, avg. train loss: 474658.56250\n",
      "Step #3200, epoch #320, avg. train loss: 463247.90625\n",
      "Step #3300, epoch #330, avg. train loss: 461680.43750\n",
      "Step #3400, epoch #340, avg. train loss: 462973.68750\n",
      "Step #3500, epoch #350, avg. train loss: 452249.12500\n",
      "Step #3600, epoch #360, avg. train loss: 449335.46875\n",
      "Step #3700, epoch #370, avg. train loss: 445718.84375\n",
      "Step #3800, epoch #380, avg. train loss: 444364.53125\n",
      "Step #3900, epoch #390, avg. train loss: 447105.87500\n",
      "Step #4000, epoch #400, avg. train loss: 439087.31250\n",
      "Step #4100, epoch #410, avg. train loss: 442407.46875\n",
      "Step #4200, epoch #420, avg. train loss: 433282.15625\n",
      "Step #4300, epoch #430, avg. train loss: 432865.96875\n",
      "Step #4400, epoch #440, avg. train loss: 427708.40625\n",
      "Step #4500, epoch #450, avg. train loss: 421188.87500\n",
      "Step #4600, epoch #460, avg. train loss: 419214.56250\n",
      "Step #4700, epoch #470, avg. train loss: 418816.18750\n",
      "Step #4800, epoch #480, avg. train loss: 418425.40625\n",
      "Step #4900, epoch #490, avg. train loss: 421692.56250\n",
      "Step #5000, epoch #500, avg. train loss: 411418.28125\n",
      "Step #100, epoch #10, avg. train loss: 692340.62500\n",
      "Step #200, epoch #20, avg. train loss: 651197.18750\n",
      "Step #300, epoch #30, avg. train loss: 633681.12500\n",
      "Step #400, epoch #40, avg. train loss: 614707.37500\n",
      "Step #500, epoch #50, avg. train loss: 605438.93750\n",
      "Step #600, epoch #60, avg. train loss: 590454.06250\n",
      "Step #700, epoch #70, avg. train loss: 579909.18750\n",
      "Step #800, epoch #80, avg. train loss: 566970.87500\n",
      "Step #900, epoch #90, avg. train loss: 560993.43750\n",
      "Step #1000, epoch #100, avg. train loss: 558324.62500\n",
      "Step #1100, epoch #110, avg. train loss: 553396.25000\n",
      "Step #1200, epoch #120, avg. train loss: 543435.93750\n",
      "Step #1300, epoch #130, avg. train loss: 527210.62500\n",
      "Step #1400, epoch #140, avg. train loss: 527295.43750\n",
      "Step #1500, epoch #150, avg. train loss: 522243.28125\n",
      "Step #1600, epoch #160, avg. train loss: 519729.15625\n",
      "Step #1700, epoch #170, avg. train loss: 510122.37500\n",
      "Step #1800, epoch #180, avg. train loss: 503303.84375\n",
      "Step #1900, epoch #190, avg. train loss: 496866.09375\n",
      "Step #2000, epoch #200, avg. train loss: 484182.18750\n",
      "Step #2100, epoch #210, avg. train loss: 487879.71875\n",
      "Step #2200, epoch #220, avg. train loss: 481097.09375\n",
      "Step #2300, epoch #230, avg. train loss: 486262.56250\n",
      "Step #2400, epoch #240, avg. train loss: 471972.96875\n",
      "Step #2500, epoch #250, avg. train loss: 471150.53125\n",
      "Step #2600, epoch #260, avg. train loss: 463595.68750\n",
      "Step #2700, epoch #270, avg. train loss: 460337.43750\n",
      "Step #2800, epoch #280, avg. train loss: 462895.18750\n",
      "Step #2900, epoch #290, avg. train loss: 456466.12500\n",
      "Step #3000, epoch #300, avg. train loss: 449381.90625\n",
      "Step #3100, epoch #310, avg. train loss: 446818.31250\n",
      "Step #3200, epoch #320, avg. train loss: 447204.71875\n",
      "Step #3300, epoch #330, avg. train loss: 436512.43750\n",
      "Step #3400, epoch #340, avg. train loss: 438306.59375\n",
      "Step #3500, epoch #350, avg. train loss: 432528.96875\n",
      "Step #3600, epoch #360, avg. train loss: 430346.87500\n",
      "Step #3700, epoch #370, avg. train loss: 430895.40625\n",
      "Step #3800, epoch #380, avg. train loss: 426994.09375\n",
      "Step #3900, epoch #390, avg. train loss: 421621.12500\n",
      "Step #4000, epoch #400, avg. train loss: 428166.03125\n",
      "Step #4100, epoch #410, avg. train loss: 420795.40625\n",
      "Step #4200, epoch #420, avg. train loss: 416517.31250\n",
      "Step #4300, epoch #430, avg. train loss: 408759.12500\n",
      "Step #4400, epoch #440, avg. train loss: 408615.37500\n",
      "Step #4500, epoch #450, avg. train loss: 401998.71875\n",
      "Step #4600, epoch #460, avg. train loss: 410381.12500\n",
      "Step #4700, epoch #470, avg. train loss: 399501.31250\n",
      "Step #4800, epoch #480, avg. train loss: 393516.56250\n",
      "Step #4900, epoch #490, avg. train loss: 399161.56250\n",
      "Step #5000, epoch #500, avg. train loss: 392412.71875\n",
      "Step #100, epoch #10, avg. train loss: 692608.37500\n",
      "Step #200, epoch #20, avg. train loss: 664721.18750\n",
      "Step #300, epoch #30, avg. train loss: 637895.18750\n",
      "Step #400, epoch #40, avg. train loss: 624920.18750\n",
      "Step #500, epoch #50, avg. train loss: 610105.25000\n",
      "Step #600, epoch #60, avg. train loss: 601720.12500\n",
      "Step #700, epoch #70, avg. train loss: 585205.37500\n",
      "Step #800, epoch #80, avg. train loss: 582314.00000\n",
      "Step #900, epoch #90, avg. train loss: 571386.81250\n",
      "Step #1000, epoch #100, avg. train loss: 561956.62500\n",
      "Step #1100, epoch #110, avg. train loss: 554923.00000\n",
      "Step #1200, epoch #120, avg. train loss: 548376.18750\n",
      "Step #1300, epoch #130, avg. train loss: 539137.25000\n",
      "Step #1400, epoch #140, avg. train loss: 530944.31250\n",
      "Step #1500, epoch #150, avg. train loss: 540459.75000\n",
      "Step #1600, epoch #160, avg. train loss: 527410.62500\n",
      "Step #1700, epoch #170, avg. train loss: 508965.75000\n",
      "Step #1800, epoch #180, avg. train loss: 509410.96875\n",
      "Step #1900, epoch #190, avg. train loss: 506941.75000\n",
      "Step #2000, epoch #200, avg. train loss: 500947.53125\n",
      "Step #2100, epoch #210, avg. train loss: 490241.96875\n",
      "Step #2200, epoch #220, avg. train loss: 494358.56250\n",
      "Step #2300, epoch #230, avg. train loss: 488320.00000\n",
      "Step #2400, epoch #240, avg. train loss: 485559.18750\n",
      "Step #2500, epoch #250, avg. train loss: 478131.56250\n",
      "Step #2600, epoch #260, avg. train loss: 473209.09375\n",
      "Step #2700, epoch #270, avg. train loss: 476155.25000\n",
      "Step #2800, epoch #280, avg. train loss: 474636.87500\n",
      "Step #2900, epoch #290, avg. train loss: 462647.59375\n",
      "Step #3000, epoch #300, avg. train loss: 458993.84375\n",
      "Step #3100, epoch #310, avg. train loss: 454161.03125\n",
      "Step #3200, epoch #320, avg. train loss: 455758.53125\n",
      "Step #3300, epoch #330, avg. train loss: 450982.68750\n",
      "Step #3400, epoch #340, avg. train loss: 450912.25000\n",
      "Step #3500, epoch #350, avg. train loss: 441386.40625\n",
      "Step #3600, epoch #360, avg. train loss: 443077.43750\n",
      "Step #3700, epoch #370, avg. train loss: 439653.28125\n",
      "Step #3800, epoch #380, avg. train loss: 429362.46875\n",
      "Step #3900, epoch #390, avg. train loss: 431970.62500\n",
      "Step #4000, epoch #400, avg. train loss: 431661.40625\n",
      "Step #4100, epoch #410, avg. train loss: 423567.46875\n",
      "Step #4200, epoch #420, avg. train loss: 432466.46875\n",
      "Step #4300, epoch #430, avg. train loss: 423523.46875\n",
      "Step #4400, epoch #440, avg. train loss: 422722.37500\n",
      "Step #4500, epoch #450, avg. train loss: 413697.00000\n",
      "Step #4600, epoch #460, avg. train loss: 418351.68750\n",
      "Step #4700, epoch #470, avg. train loss: 410065.40625\n",
      "Step #4800, epoch #480, avg. train loss: 403899.12500\n",
      "Step #4900, epoch #490, avg. train loss: 402538.53125\n",
      "Step #5000, epoch #500, avg. train loss: 403686.25000\n",
      "10-fold CV Acc Mean:  -0.539266377985\n",
      "CV Scores:  -0.671179800701, -0.607976293156, -0.608522157224, -0.418605467432, -0.506637203441, -0.658635901229, -0.533312636398, -0.393844325048, -0.432054229548, -0.561895765674\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TensorFlowEstimator(batch_size=100, class_weight=None, clip_gradients=5.0,\n",
       "          config=None, continue_training=False, learning_rate=0.1,\n",
       "          model_fn=<function tanh_dnn at 0x7febc38ea9b0>, n_classes=0,\n",
       "          optimizer='Adagrad', steps=5000, verbose=1)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.dnn_cross_val(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
